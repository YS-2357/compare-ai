# 2025-12-18 개발 기록

## 백엔드
- 프롬프트 평가 스트리머 신설 (`app/services/prompt_eval.py`)
  - 질문과 공통 프롬프트를 받아 병렬로 모델 호출 → 선택된 벤더의 최신 모델(예: gpt-5-mini, gemini-3-flash-preview, claude-sonnet-4-5-20250929, solar-mini, sonar 등)을 평가모델로 호출해 블라인드 스코어링.
  - 평가 출력 스키마(Pydantic `ScoreList`)로 파싱, 실패 시 score=-1/rationale="parse failed"로 fallback.
  - summary 이벤트에 `scores`(JSON), `avg_score`, `evaluations`(평가자별 상태/소요시간/점수), `elapsed_ms` 포함(마크다운 테이블 제거).
- API 스키마/라우팅
  - `PromptEvalRequest` 추가(`app/api/schemas/prompt_eval.py`): `question`, `prompt`(공통), `models` 리스트.
  - `/api/prompt-eval` 스트리밍 엔드포인트 추가(`app/api/routes.py`), 인증/일일 한도 동일 적용, NDJSON partial/summary/usage 이벤트 반환.
- 오류/상태 정리
  - 모델 호출 실패 시 통일된 메시지("응답 실패: ...")와 status/error 필드 유지, raw_responses/raw_sources에 기록.

## 프런트엔드(UI)
- Streamlit 새 탭 “프롬프트 평가”
  - 질문 입력, 모델 멀티선택, 공통 프롬프트 입력(UI 기본값: `[Question]\n{question}\n\n답변은 한국어로 작성하세요.`) 제공.
  - `_send_prompt_eval`로 `/api/prompt-eval` 스트림 소비: partial 응답 모델별 표시, 평가 summary에서 점수 DataFrame/평균 출력, usage 이벤트 무시.
  - 평가자별 상태/소요시간/점수 수를 expander 테이블로 표시하고, 원본 JSON을 함께 제공.
  - 세션에 `prompt_eval_log` 추가로 실행 이력 저장.
- 기존 대화 탭 유지, 공통 에러 표시: status 기반으로 error 시 `st.error` 사용.

## 로깅
- `app/logger.py` 콘솔 포맷 ANSI 색상 + 이모지로 교체(파일 핸들러 없음, 기존 구성 유지).

## 테스트
- `python3 -m py_compile` 실행 대상으로 새 모듈 포함: `app/services/prompt_eval.py`, `app/api/schemas/prompt_eval.py`, `app/ui/streamlit_app.py`.
