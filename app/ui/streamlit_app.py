"""Streamlit UI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸."""

from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Any

import requests
import streamlit as st
from dotenv import load_dotenv
from app.utils.logger import get_logger

load_dotenv()
logger = get_logger(__name__)

FASTAPI_URL_FILE = Path(__file__).resolve().parents[2] / ".fastapi_url"
DEFAULT_FASTAPI_BASE = FASTAPI_URL_FILE.read_text().strip() if FASTAPI_URL_FILE.exists() else ""
MODEL_OPTIONS: dict[str, dict[str, Any]] = {
    "openai": {
        "label": "OpenAI",
        "env": "MODEL_OPENAI",
        "choices": ["gpt-4o-mini", "gpt-4o-nano", "gpt-4o"],
    },
    "gemini": {
        "label": "Google Gemini",
        "env": "MODEL_GEMINI",
        "choices": ["gemini-2.5-flash-lite", "gemini-2.0-flash", "gemini-1.5-pro"],
    },
    "anthropic": {
        "label": "Anthropic Claude",
        "env": "MODEL_ANTHROPIC",
        "choices": [
            "claude-haiku-4-5-20251001",
            "claude-3-5-sonnet-20241022",
            "claude-3-opus-20240229",
        ],
    },
    "upstage": {
        "label": "Upstage Solar",
        "env": "MODEL_UPSTAGE",
        "choices": ["solar-mini", "solar-pro", "solar-1-mini-chat"],
    },
    "perplexity": {
        "label": "Perplexity Sonar",
        "env": "MODEL_PERPLEXITY",
        "choices": ["sonar", "sonar-pro", "sonar-medium-online"],
    },
    "mistral": {
        "label": "Mistral",
        "env": "MODEL_MISTRAL",
        "choices": ["mistral-large-latest", "mistral-large-2407", "mistral-small-latest"],
    },
    "groq": {
        "label": "Groq",
        "env": "MODEL_GROQ",
        "choices": ["llama3-70b-8192", "llama3-8b-8192", "mixtral-8x7b-32768"],
    },
    "cohere": {
        "label": "Cohere",
        "env": "MODEL_COHERE",
        "choices": ["command-r-plus", "command-r", "command-r7b"],
    },
}

st.set_page_config(page_title="Compare-AI", page_icon="ðŸ¤–", layout="wide")


def _default_model(provider: str) -> str:
    logger.debug("_default_model:ì‹œìž‘ provider=%s", provider)
    meta = MODEL_OPTIONS[provider]
    env_value = os.getenv(meta["env"])
    if env_value:
        logger.info("_default_model:í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© provider=%s model=%s", provider, env_value)
        return env_value
    # ê°€ë²¼ìš´/ì €ë ´í•œ ëª¨ë¸ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„ íƒ(ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ì²« ë²ˆì§¸)
    cheap_candidates = [
        "gpt-4o-mini",
        "gpt-4o-nano",
        "gpt-5-nano",
        "gemini-2.5-flash-lite",
        "claude-3-haiku",
        "solar-mini",
        "sonar-small",
        "mistral-small-3.2",
        "ministral-3-8b",
        "grok-2-mini",
        "command-light",
    ]
    for candidate in cheap_candidates:
        if candidate in meta["choices"]:
            logger.debug("_default_model:ì €ê°€í˜• ì„ íƒ provider=%s model=%s", provider, candidate)
            return candidate
    chosen = meta["choices"][0]
    logger.debug("_default_model:ê¸°ë³¸ ì„ íƒ provider=%s model=%s", provider, chosen)
    return chosen


def _ensure_model_selections() -> None:
    logger.debug("_ensure_model_selections:ì‹œìž‘")
    defaults = {key: _default_model(key) for key in MODEL_OPTIONS}
    selections = st.session_state.get("model_selections") or {}
    merged = {}
    for key, default in defaults.items():
        merged[key] = selections.get(key, default)
    st.session_state["model_selections"] = merged
    logger.debug("_ensure_model_selections:ì¢…ë£Œ selections=%s", merged)


def _render_model_selector() -> None:
    logger.debug("_render_model_selector:ì‹œìž‘")
    _ensure_model_selections()
    st.subheader("ëª¨ë¸ ì„ íƒ")
    for key, meta in MODEL_OPTIONS.items():
        options = list(meta["choices"])
        current = st.session_state["model_selections"].get(key, _default_model(key))
        if current not in options:
            options = [current] + options
        index = options.index(current) if current in options else 0
        selection = st.selectbox(
            f"{meta['label']} ëª¨ë¸",
            options,
            index=index,
            key=f"model_select_{key}",
        )
        st.session_state["model_selections"][key] = selection
    logger.debug("_render_model_selector:ì¢…ë£Œ")


def _load_base_url() -> str:
    logger.debug("_load_base_url:ì‹œìž‘")
    saved = (
        st.session_state.get("fastapi_base_url")
        or DEFAULT_FASTAPI_BASE
        or os.getenv("FASTAPI_URL", "")
        or st.secrets.get("FASTAPI_URL", "")
    )
    if saved.endswith("/api/ask"):
        base = saved.rsplit("/api/ask", 1)[0]
    else:
        base = saved
    logger.debug("_load_base_url:ì¢…ë£Œ base=%s", base)
    return base


def _get_usage_limit() -> str:
    value = os.getenv("DAILY_USAGE_LIMIT") or "3"
    logger.debug("_get_usage_limit:limit=%s", value)
    return value


def _usage_limit_int() -> int:
    try:
        result = int(_get_usage_limit())
        logger.debug("_usage_limit_int:ì„±ê³µ value=%s", result)
        return result
    except Exception:
        logger.warning("_usage_limit_int:ë³€í™˜ ì‹¤íŒ¨, ê¸°ë³¸ê°’ 3 ì‚¬ìš©")
        return 3


def _sync_usage_from_headers(resp: requests.Response) -> None:
    """ì„œë²„ í—¤ë”ì˜ ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ì„¸ì…˜ì— ë°˜ì˜í•œë‹¤."""

    limit = resp.headers.get("X-Usage-Limit")
    remaining = resp.headers.get("X-Usage-Remaining")
    if limit is not None and limit.isdigit():
        st.session_state["usage_limit"] = int(limit)
    if remaining is not None and remaining.isdigit():
        st.session_state["usage_remaining"] = int(remaining)
    logger.debug(
        "_sync_usage_from_headers:limit=%s remaining=%s", st.session_state.get("usage_limit"), st.session_state.get("usage_remaining")
    )


def _build_history_payload(chat_log: list[dict[str, Any]]) -> list[dict[str, str]]:
    """ê¸°ì¡´ ëŒ€í™” ë¡œê·¸ë¥¼ LangGraph history íŽ˜ì´ë¡œë“œë¡œ ë³€í™˜í•œë‹¤."""

    logger.debug("_build_history_payload:ì‹œìž‘ entries=%d", len(chat_log or []))
    history_payload: list[dict[str, str]] = []
    for entry in chat_log or []:
        q = entry.get("question")
        if q:
            history_payload.append({"role": "user", "content": q})
        model_answers: dict[str, str] = {}
        for model, ans in (entry.get("answers") or {}).items():
            if ans:
                model_answers[model] = ans
        if not model_answers:
            for ev in entry.get("events") or []:
                model = ev.get("model")
                ans = ev.get("answer")
                if model and ans and model not in model_answers:
                    model_answers[model] = ans
        for model, ans in model_answers.items():
            history_payload.append({"role": "assistant", "model": model, "content": ans})
    logger.debug("_build_history_payload:ì¢…ë£Œ payload_len=%d", len(history_payload))
    return history_payload


def _update_usage_after_response(resp: requests.Response, *, admin_mode: bool) -> None:
    """ì‘ë‹µ ì´í›„ ì‚¬ìš©ëŸ‰ ì¹´ìš´í„°ë¥¼ ê°±ì‹ í•œë‹¤."""

    logger.debug("_update_usage_after_response:ì‹œìž‘ admin_mode=%s status=%s", admin_mode, resp.status_code)
    if admin_mode:
        st.session_state["usage_remaining"] = None
        return
    if resp.status_code == 429:
        st.session_state["usage_remaining"] = 0
    elif resp.ok:
        if "X-Usage-Remaining" not in resp.headers:
            new_value = max(0, st.session_state.get("usage_remaining", _usage_limit_int()) - 1)
            st.session_state["usage_remaining"] = new_value
    logger.debug(
        "_update_usage_after_response:ì¢…ë£Œ usage_remaining=%s", st.session_state.get("usage_remaining")
    )


def _append_chat_log_entry(
    question: str,
    answers: dict[str, str],
    sources: dict[str, str | None],
    events: list[dict[str, Any]],
) -> None:
    """ëŒ€í™” ë¡œê·¸ì— ìƒˆ ì—”íŠ¸ë¦¬ë¥¼ ì¶”ê°€í•œë‹¤."""

    logger.debug("_append_chat_log_entry:ì‹œìž‘ question=%s answers=%d events=%d", question, len(answers), len(events))
    st.session_state["chat_log"].append(
        {
            "question": question,
            "answers": answers,
            "sources": sources,
            "events": events,
        }
    )
    logger.debug("_append_chat_log_entry:ì¢…ë£Œ total=%d", len(st.session_state["chat_log"]))


def _status_to_emoji(status_val: Any) -> str:
    """ìƒíƒœ ì½”ë“œ/ë¬¸ìžì—´ì„ ì´ëª¨ì§€ë¡œ ë³€í™˜í•œë‹¤."""

    logger.debug("_status_to_emoji:ì‹œìž‘ status=%s", status_val)
    code = None
    if isinstance(status_val, dict):
        code = status_val.get("status")
    elif isinstance(status_val, (int, str)):
        code = status_val

    if isinstance(code, str):
        code_lower = code.lower()
        if code_lower.isdigit():
            code = int(code_lower)
        elif "error" in code_lower or "fail" in code_lower or "exception" in code_lower:
            return "âŒ"
        elif "timeout" in code_lower or "rate" in code_lower:
            return "âš ï¸"
        elif "ok" in code_lower or "success" in code_lower:
            return "âœ…"

    try:
        code_int = int(code) if code is not None else None
    except Exception:
        code_int = None
    if code_int is None:
        logger.debug("_status_to_emoji:ì¢…ë£Œ emoji=â”")
        return "â”"
    if code_int >= 500:
        logger.debug("_status_to_emoji:ì¢…ë£Œ emoji=âŒ")
        return "âŒ"
    if code_int >= 400:
        logger.debug("_status_to_emoji:ì¢…ë£Œ emoji=âš ï¸")
        return "âš ï¸"
    logger.debug("_status_to_emoji:ì¢…ë£Œ emoji=âœ…")
    return "âœ…"


def _is_error_status(status_val: Any) -> bool:
    """ìƒíƒœ ì½”ë“œ/ë¬¸ìžì—´ì´ ì˜¤ë¥˜ì¸ì§€ íŒë³„í•œë‹¤."""

    logger.debug("_is_error_status:ì‹œìž‘ status=%s", status_val)
    code = None
    if isinstance(status_val, dict):
        code = status_val.get("status")
    elif isinstance(status_val, (int, str)):
        code = status_val

    if isinstance(code, str):
        lower = code.lower()
        if lower.isdigit():
            code = int(lower)
        elif "error" in lower or "fail" in lower or "exception" in lower:
            return True
        elif "timeout" in lower or "rate" in lower:
            return True

    try:
        code_int = int(code) if code is not None else None
    except Exception:
        code_int = None
    if code_int is None:
        logger.debug("_is_error_status:ì¢…ë£Œ result=False")
        return False
    result = code_int >= 400
    logger.debug("_is_error_status:ì¢…ë£Œ result=%s", result)
    return result


def _render_auth_section(base_url: str) -> None:
    """ë¡œê·¸ì¸/íšŒì›ê°€ìž… UIë¥¼ ë Œë”ë§í•œë‹¤."""

    logger.debug("_render_auth_section:ì‹œìž‘ base_url=%s", base_url)
    st.header("ë¡œê·¸ì¸ ë˜ëŠ” íšŒì›ê°€ìž…")
    email = st.text_input("ì´ë©”ì¼")
    password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
    col1, col2 = st.columns(2)
    with col1:
        if st.button("íšŒì›ê°€ìž…"):
            if not base_url:
                st.error("FastAPI Base URLì„ ìž…ë ¥í•˜ì„¸ìš”.")
            elif not email or not password:
                st.warning("ì´ë©”ì¼/ë¹„ë°€ë²ˆí˜¸ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.")
            else:
                try:
                    resp = requests.post(
                        f"{base_url}/auth/register",
                        json={"email": email, "password": password},
                        timeout=15,
                    )
                    st.write(f"íšŒì›ê°€ìž… ìƒíƒœ: {resp.status_code}")
                    st.json(resp.json())
                except Exception as exc:
                    logger.error("_render_auth_section:íšŒì›ê°€ìž… ì‹¤íŒ¨ email=%s err=%s", email, exc)
                    st.error(f"íšŒì›ê°€ìž… ì‹¤íŒ¨: {exc}")
    with col2:
        if st.button("ë¡œê·¸ì¸"):
            if not base_url:
                st.error("FastAPI Base URLì„ ìž…ë ¥í•˜ì„¸ìš”.")
            elif not email or not password:
                st.warning("ì´ë©”ì¼/ë¹„ë°€ë²ˆí˜¸ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.")
            else:
                try:
                    resp = requests.post(
                        f"{base_url}/auth/login",
                        json={"email": email, "password": password},
                        timeout=15,
                    )
                    data = resp.json()
                    st.write(f"ë¡œê·¸ì¸ ìƒíƒœ: {resp.status_code}")
                    if resp.ok and data.get("access_token"):
                        token = f"{data.get('token_type', 'bearer')} {data['access_token']}"
                        st.session_state["auth_token"] = token
                        st.session_state["auth_user"] = data.get("user")
                        st.success("ë¡œê·¸ì¸ ì„±ê³µ: í† í° ì €ìž¥ ì™„ë£Œ")
                        st.rerun()
                    st.json(data)
                except Exception as exc:
                    logger.error("_render_auth_section:ë¡œê·¸ì¸ ì‹¤íŒ¨ email=%s err=%s", email, exc)
                    st.error(f"ë¡œê·¸ì¸ ì‹¤íŒ¨: {exc}")
    logger.debug("_render_auth_section:ì¢…ë£Œ")
    st.stop()


def _render_chat_history(chat_log: list[dict[str, Any]]) -> None:
    """ê¸°ì¡´ ëŒ€í™” ë¡œê·¸ë¥¼ ì±—ë´‡ í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•œë‹¤."""

    logger.debug("_render_chat_history:ì‹œìž‘ entries=%d", len(chat_log or []))
    if not chat_log:
        st.info("ì•„ì§ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ìž…ë ¥í•´ë³´ì„¸ìš”.")
        return

    for item in chat_log:
        with st.chat_message("user"):
            st.write(item.get("question"))
        answers = item.get("answers") or {}
        sources = item.get("sources") or {}
        events = item.get("events") or []
        # ëª¨ë¸ë³„ ìƒíƒœ/ì‹œê°„ ë©”íƒ€ êµ¬ì„±
        event_meta: dict[str, dict[str, Any]] = {}
        for ev in events:
            model = ev.get("model")
            if not model:
                continue
            event_meta[model] = {
                "status": ev.get("status"),
                "elapsed_ms": ev.get("elapsed_ms"),
            }
        with st.chat_message("assistant"):
            if answers:
                for model, answer in answers.items():
                    meta = event_meta.get(model) or {}
                    status = meta.get("status")
                    elapsed_ms = meta.get("elapsed_ms")
                    emoji = _status_to_emoji(status)
                    elapsed_txt = f"{elapsed_ms/1000:.1f}s" if elapsed_ms is not None else "-"
                    st.markdown(f"{emoji} **{model}** â±ï¸ {elapsed_txt}")
                    st.write(answer)
                    src = sources.get(model)
                    if model == "Perplexity":
                        st.caption(f"ì¶œì²˜: {src or 'ì œê³µë˜ì§€ ì•ŠìŒ'}")
                    elif src:
                        st.caption(f"ì¶œì²˜: {src}")
            elif events:
                st.caption("ì‘ë‹µ ìŠ¤íŠ¸ë¦¼")
                for ev in events:
                    model = ev.get("model") or "unknown"
                    ans = ev.get("answer")
                    src = ev.get("source")
                    status = ev.get("status") or {}
                    elapsed = ev.get("elapsed_ms")
                    elapsed_txt = f"{elapsed/1000:.1f}s" if elapsed is not None else "-"
                    emoji = _status_to_emoji(status)
                    st.write(f"{emoji} [{model}] {ans}")
                    st.caption(f"â±ï¸ {elapsed_txt}")
                    if model == "Perplexity":
                        st.caption(f"ì¶œì²˜: {src or 'ì œê³µë˜ì§€ ì•ŠìŒ'}")
                    elif src:
                        st.caption(f"ì¶œì²˜: {src}")
    logger.debug("_render_chat_history:ì¢…ë£Œ")


def _render_connection_status(base_url: str) -> None:
    """API ì—°ê²° ìƒíƒœë¥¼ ê°„ë‹¨ížˆ í‘œì‹œí•œë‹¤."""

    logger.debug("_render_connection_status:ì‹œìž‘ base_url=%s", base_url)
    status_box = st.empty()
    if not base_url:
        status_box.warning("FastAPI URLì„ ìž…ë ¥í•˜ì„¸ìš”.")
        return
    with st.spinner("API ì—°ê²° í™•ì¸ ì¤‘..."):
        try:
            resp = requests.get(f"{base_url}/health", timeout=5)
            if resp.ok:
                status_box.success("âœ… API ì—°ê²°ë¨")
            else:
                status_box.error(f"âŒ API ì‘ë‹µ ì˜¤ë¥˜ ({resp.status_code})")
        except Exception as exc:  # pragma: no cover - UI í†µì‹  ì˜ˆì™¸
            status_box.error(f"âŒ ì—°ê²° ì‹¤íŒ¨: {exc}")
            logger.error("_render_connection_status:ì‹¤íŒ¨ err=%s", exc)
    logger.debug("_render_connection_status:ì¢…ë£Œ")


def _handle_logout() -> None:
    """ë¡œê·¸ì•„ì›ƒ ì²˜ë¦¬."""

    logger.debug("_handle_logout:ì‹œìž‘")
    st.session_state.pop("auth_token", None)
    st.session_state.pop("auth_user", None)
    st.session_state.pop("usage_remaining", None)
    st.session_state.pop("usage_bypass", None)
    st.session_state.pop("usage_fetched", None)
    st.session_state.pop("chat_log", None)
    st.rerun()


def _send_question(
    question: str,
    ask_url: str,
    headers: dict[str, str],
    turn_value: int,
    history_payload: list[dict[str, str]],
    model_overrides: dict[str, str] | None = None,
) -> None:
    """ì§ˆë¬¸ì„ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ì„¸ì…˜ì— ë°˜ì˜í•œë‹¤."""

    logger.debug("_send_question:ì‹œìž‘ question=%s turn=%s", question, turn_value)
    payload: dict[str, Any] = {"question": question, "turn": turn_value, "history": history_payload}
    if model_overrides:
        payload["models"] = {k: v for k, v in model_overrides.items() if v}
    resp = requests.post(ask_url, headers=headers, json=payload, stream=True, timeout=60)
    _sync_usage_from_headers(resp)

    live_key = f"live_{turn_value}"
    placeholders = {}
    events_acc: list[dict[str, Any]] = []
    answers_acc: dict[str, str] = {}
    sources_acc: dict[str, str | None] = {}

    for line in resp.iter_lines():
        if not line:
            continue
        try:
            parsed = json.loads(line.decode("utf-8"))
        except Exception:
            parsed = line
        if not isinstance(parsed, dict):
            continue
        event_type = parsed.get("type", "partial")
        if event_type == "partial":
            model = parsed.get("model")
            if not model:
                continue
            status = parsed.get("status") or {}
            answer = parsed.get("answer")
            source = parsed.get("source")
            answers_acc[model] = answer
            sources_acc[model] = source
            events_acc.append(
                {
                    "model": model,
                    "answer": answer,
                    "source": source,
                    "status": status,
                    "elapsed_ms": parsed.get("elapsed_ms"),
                }
            )
            if model not in placeholders:
                placeholders[model] = st.empty()
            slot = placeholders[model]
            with slot.container():
                elapsed = parsed.get("elapsed_ms")
                elapsed_txt = f"{elapsed/1000:.1f}s" if elapsed is not None else "-"
                emoji = _status_to_emoji(status)
                st.markdown(f"{emoji} **{model}** â±ï¸ {elapsed_txt}")
                if _is_error_status(status):
                    st.error(answer or "ì‘ë‹µì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.write(answer)
                src = source
                if model == "Perplexity":
                    st.caption(f"ì¶œì²˜: {src or 'ì œê³µë˜ì§€ ì•ŠìŒ'}")
                elif src:
                    st.caption(f"ì¶œì²˜: {src}")
        elif event_type == "error":
            model = parsed.get("model") or "unknown"
            message = parsed.get("message") or "ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            status = parsed.get("status") or {}
            elapsed = parsed.get("elapsed_ms")
            elapsed_txt = f"{elapsed/1000:.1f}s" if elapsed is not None else "-"
            emoji = _status_to_emoji(status or "error")
            events_acc.append(
                {
                    "model": model,
                    "answer": message,
                    "source": None,
                    "status": status or "error",
                    "elapsed_ms": elapsed,
                }
            )
            if model not in placeholders:
                placeholders[model] = st.empty()
            slot = placeholders[model]
            with slot.container():
                st.markdown(f"{emoji} **{model}** â±ï¸ {elapsed_txt}")
                st.error(message)
        elif event_type == "summary":
            result = parsed.get("result") or {}
            answers_acc = result.get("answers") or answers_acc
            sources_acc = result.get("sources") or sources_acc
            turn = result.get("turn", turn_value)
            max_turns = result.get("max_turns")
            usage_remaining = result.get("usage_remaining")
            if usage_remaining is not None:
                st.session_state["usage_remaining"] = usage_remaining
            _append_chat_log_entry(question, answers_acc, sources_acc, events_acc)
            _update_usage_after_response(resp, admin_mode=st.session_state.get("usage_bypass"))
            logger.info("_send_question:ìš”ì•½ ìˆ˜ì‹  turn=%s max_turns=%s answers=%d", turn, max_turns, len(answers_acc))
            st.rerun()
            return

    # ìš”ì•½ì´ ì•ˆ ì™”ì„ ë•Œë„ ê¸°ë¡ë§Œ ë‚¨ê¹€
    _append_chat_log_entry(question, answers_acc, sources_acc, events_acc)
    _update_usage_after_response(resp, admin_mode=st.session_state.get("usage_bypass"))
    logger.warning("_send_question:ìš”ì•½ ë¯¸ìˆ˜ì‹ , ê¸°ë¡ë§Œ ì €ìž¥")
    st.rerun()


def _send_prompt_eval(
    question: str,
    eval_url: str,
    headers: dict[str, str],
    prompt_payload: str | None,
    active_models: list[str],
) -> None:
    """í”„ë¡¬í”„íŠ¸ í‰ê°€ ìš”ì²­ì„ ì „ì†¡í•˜ê³  ìŠ¤íŠ¸ë¦¼ ì‘ë‹µì„ í‘œì‹œí•œë‹¤."""

    logger.debug("_send_prompt_eval:ì‹œìž‘ question=%s models=%s", question, active_models)
    payload: dict[str, Any] = {"question": question, "models": active_models}
    if prompt_payload:
        payload["prompt"] = prompt_payload
    reference_answer = st.session_state.get("prompt_eval_reference") or ""
    if reference_answer.strip():
        payload["reference_answer"] = reference_answer.strip()

    resp = requests.post(eval_url, headers=headers, json=payload, stream=True, timeout=120)
    placeholders: dict[str, Any] = {}
    events_acc: list[dict[str, Any]] = []
    summary_data: dict[str, Any] | None = None

    for line in resp.iter_lines():
        if not line:
            continue
        try:
            parsed = json.loads(line.decode("utf-8"))
        except Exception:
            parsed = line
        if not isinstance(parsed, dict):
            continue
        event_type = parsed.get("type", "partial")
        if event_type == "partial":
            model = parsed.get("model")
            if not model:
                continue
            status = parsed.get("status") or {}
            answer = parsed.get("answer")
            elapsed = parsed.get("elapsed_ms")
            elapsed_txt = f"{elapsed/1000:.1f}s" if elapsed is not None else "-"
            emoji = _status_to_emoji(status)
            events_acc.append(
                {
                    "model": model,
                    "answer": answer,
                    "source": None,
                    "status": status,
                    "elapsed_ms": elapsed,
                }
            )
            if model not in placeholders:
                placeholders[model] = st.empty()
            with placeholders[model].container():
                st.markdown(f"{emoji} **{model}** â±ï¸ {elapsed_txt}")
                if _is_error_status(status):
                    st.error(answer or "ì‘ë‹µì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.write(answer)
        elif event_type == "error":
            message = parsed.get("message") or "ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            st.error(message)
            logger.error("_send_prompt_eval:ì—ëŸ¬ ì´ë²¤íŠ¸ model=%s message=%s", parsed.get("model"), message)
            events_acc.append(
                {
                    "model": parsed.get("model") or "unknown",
                    "answer": message,
                    "source": None,
                    "status": parsed.get("status") or "error",
                    "elapsed_ms": parsed.get("elapsed_ms"),
                }
            )
        elif event_type == "summary":
            summary_data = parsed.get("result") or {}
            scores = summary_data.get("scores") or []
            avg_score = summary_data.get("avg_score")
            logger.info("_send_prompt_eval:ìš”ì•½ ìˆ˜ì‹  scores=%d avg=%s", len(scores), avg_score)
            st.subheader("ðŸ í‰ê°€ ê²°ê³¼")
            if avg_score is not None:
                st.markdown(f"âœ¨ **í‰ê·  ì ìˆ˜:** {avg_score}")
            if scores:
                # ì¹´ë“œí˜• ìš”ì•½ (ëª¨ë¸/ì ìˆ˜/ìˆœìœ„/ìƒíƒœ/ê·¼ê±°)
                for i in range(0, len(scores), 2):
                    cols = st.columns(2)
                    for score_row, col in zip(scores[i : i + 2], cols):
                        model = score_row.get("model")
                        score_val = score_row.get("score")
                        rank = score_row.get("rank")
                        rationale = score_row.get("rationale") or ""
                        status_val = score_row.get("status") or {}
                        emoji = _status_to_emoji(status_val)
                        with col.container():
                            st.markdown(f"{emoji} **{model}** â€” ì ìˆ˜: {score_val}, ìˆœìœ„: {rank}")
                            if rationale:
                                st.write(rationale)
                # ë³µì‚¬ìš© í…ìŠ¤íŠ¸ ë·°
                lines = []
                for s in scores:
                    lines.append(f"[{s.get('model')}] score={s.get('score')} rank={s.get('rank')}")
                    if s.get("rationale"):
                        lines.append(f"rationale: {s.get('rationale')}")
                st.markdown("ðŸ“‹ ë³µì‚¬ìš© í…ìŠ¤íŠ¸")
                st.code("\n".join(lines), language="text")
                st.download_button(
                    "ê²°ê³¼ JSON ë‹¤ìš´ë¡œë“œ",
                    data=json.dumps(summary_data, ensure_ascii=False, indent=2),
                    file_name="prompt_eval_result.json",
                    mime="application/json",
                )
            evaluations = summary_data.get("evaluations") or []
            if evaluations:
                with st.expander("ðŸ§  í‰ê°€ìžë³„ ì›ë³¸ ì ìˆ˜/ê·¼ê±° ë³´ê¸°", expanded=False):
                    for ev in evaluations:
                        status = ev.get("status") or {}
                        status_str = ""
                        if isinstance(status, dict):
                            status_str = str(status.get("status") or status)
                        elif status is not None:
                            status_str = str(status)
                        emoji = _status_to_emoji(status)
                        st.markdown(
                            f"{emoji} **í‰ê°€ìž:** {ev.get('evaluator')} | ìƒíƒœ: {status_str} | ëª¨ë¸: "
                            f"{(status.get('model') if isinstance(status, dict) else '')}"
                        )
                        score_list = ev.get("scores") or []
                        for sc in score_list:
                            st.write(f"- ëŒ€ìƒ: {sc.get('model')} | ì ìˆ˜: {sc.get('score')} | ìˆœìœ„: {sc.get('rank')}")
                            if sc.get("rationale"):
                                st.caption(f"  ê·¼ê±°: {sc.get('rationale')}")
                        if ev.get("elapsed_ms") is not None:
                            st.caption(f"ì†Œìš” ì‹œê°„: {ev.get('elapsed_ms')} ms")
        elif event_type == "usage":
            # ì‚¬ìš©ëŸ‰ ë©”íƒ€ëŠ” í‘œì‹œë§Œ ê±´ë„ˆëœ€
            continue

    if summary_data:
        # ê°„ë‹¨í•œ ë¡œê·¸ ì €ìž¥
        st.session_state.setdefault("prompt_eval_log", []).append(
            {
                "question": question,
                "events": events_acc,
                "summary": summary_data,
            }
        )
    logger.debug("_send_prompt_eval:ì¢…ë£Œ")


def main() -> None:
    logger.debug("streamlit_main:ì‹œìž‘")
    st.title("Compare-AI")
    st.caption("ì—¬ëŸ¬ LLM ì¤‘ ë‚´ ì§ˆë¬¸ì— ê°€ìž¥ ìž˜ ë‹µí•˜ëŠ” ëª¨ë¸ì„ ì°¾ì•„ë³´ì„¸ìš”.")

    with st.sidebar:
        base_url = _load_base_url().rstrip("/")
        st.session_state["fastapi_base_url"] = base_url
        st.text_input("FastAPI URL", value=base_url or "í™˜ê²½ë³€ìˆ˜/íŒŒì¼ë¡œ ì„¤ì •í•˜ì„¸ìš”", disabled=True)
        if not base_url:
            st.error("FASTAPI_URL í™˜ê²½ë³€ìˆ˜ë‚˜ .fastapi_url íŒŒì¼ë¡œ ë°±ì—”ë“œ ì£¼ì†Œë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            st.stop()
        _render_connection_status(base_url)
        _render_model_selector()

    ask_url = f"{base_url}/api/ask" if base_url else ""
    eval_url = f"{base_url}/api/prompt-eval" if base_url else ""

    # ì¸ì¦/íšŒì›ê°€ìž… ë·°
    if not st.session_state.get("auth_token"):
        _render_auth_section(base_url)

    # ì„¸ì…˜ ê¸°ë³¸ê°’
    if "usage_remaining" not in st.session_state:
        st.session_state["usage_remaining"] = _usage_limit_int()
    if "usage_bypass" not in st.session_state:
        st.session_state["usage_bypass"] = False
    if "chat_log" not in st.session_state:
        st.session_state["chat_log"] = []
    if "prompt_eval_log" not in st.session_state:
        st.session_state["prompt_eval_log"] = []

    # ë¡œê·¸ì¸ í›„ ìµœì´ˆ 1íšŒ ì‚¬ìš©ëŸ‰ ì¡°íšŒ
    if st.session_state.get("auth_token") and "usage_fetched" not in st.session_state:
        usage_url = f"{base_url}/usage"
        try:
            resp = requests.get(usage_url, headers={"Authorization": st.session_state["auth_token"]}, timeout=5)
            data = resp.json()
            if resp.ok:
                remaining_val = data.get("remaining")
                if data.get("bypass"):
                    st.session_state["usage_remaining"] = None
                    st.session_state["usage_bypass"] = True
                elif isinstance(remaining_val, int):
                    st.session_state["usage_remaining"] = remaining_val
                    st.session_state["usage_bypass"] = False
            st.session_state["usage_fetched"] = True
        except Exception:
            st.session_state["usage_fetched"] = True
    if user := st.session_state.get("auth_user"):
        st.caption(f"ë¡œê·¸ì¸ë¨: {user.get('email')}")
    if st.session_state.get("usage_bypass"):
        st.caption("ê´€ë¦¬ìž ê¶Œí•œ í™œì„±í™” (ì¼ì¼ ì œí•œ ì—†ìŒ)")
    remaining = st.session_state.get("usage_remaining")
    if remaining is None:
        st.success("ë‚¨ì€ ì¼ì¼ ì‚¬ìš© íšŸìˆ˜: ë¬´ì œí•œ (ê´€ë¦¬ìž ëª¨ë“œ)")
    elif remaining == 0:
        st.error("ë‚¨ì€ ì¼ì¼ ì‚¬ìš© íšŸìˆ˜: **0íšŒ** (ê´€ë¦¬ìž ìš°íšŒ í•„ìš”)")
    else:
        st.info(f"ë‚¨ì€ ì¼ì¼ ì‚¬ìš© íšŸìˆ˜: **{remaining}íšŒ** (ê´€ë¦¬ìž ìš°íšŒ ì‹œ ì œí•œ ì—†ìŒ)")
    if st.button("ë¡œê·¸ì•„ì›ƒ"):
        _handle_logout()

    logger.debug("streamlit_main:ì¢…ë£Œ")

    tab_compare, tab_prompt = st.tabs(["ëª¨ë¸ ë¹„êµ", "í”„ë¡¬í”„íŠ¸ í‰ê°€"])
    tab_graph = st.tabs(["ê·¸ëž˜í”„ ë³´ê¸°"])[0]

    with tab_compare:
        st.header("ëŒ€í™”")
        _render_chat_history(st.session_state["chat_log"])

        question = st.chat_input("ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”...")

        if question:
            if not ask_url:
                st.error("FastAPI Base URLì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
                return
            headers = {"Content-Type": "application/json"}
            if token := st.session_state.get("auth_token"):
                headers["Authorization"] = token
            history_payload = _build_history_payload(st.session_state.get("chat_log", []))
            model_overrides = st.session_state.get("model_selections")
            turn_value = len(st.session_state.get("chat_log", [])) + 1

            with st.spinner("ëª¨ë¸ ë¹„êµ ì¤‘..."):
                try:
                    _send_question(question, ask_url, headers, turn_value, history_payload, model_overrides=model_overrides)
                except Exception as exc:  # pragma: no cover - UI ì˜ˆì™¸
                    st.error(f"ìš”ì²­ ì‹¤íŒ¨: {exc}")

    with tab_prompt:
        st.header("í”„ë¡¬í”„íŠ¸ í‰ê°€")
        st.write("ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ë¥¼ ë‹¤ë¥´ê²Œ ì ìš©í•´ ì‘ë‹µì„ ë°›ê³ , ê³ ì • í‰ê°€ëª¨ë¸ë¡œ ë¸”ë¼ì¸ë“œ í‰ê°€í•©ë‹ˆë‹¤.")
        if not eval_url:
            st.error("FastAPI Base URLì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            return
        headers = {"Content-Type": "application/json"}
        if token := st.session_state.get("auth_token"):
            headers["Authorization"] = token

        active_models = st.multiselect(
            "í‰ê°€í•  ëª¨ë¸ ì„ íƒ",
            options=list(MODEL_OPTIONS.keys()),
            default=list(MODEL_OPTIONS.keys()),
        )
        question_eval = st.text_area("ì§ˆë¬¸", placeholder="ë¹„êµí•  ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”", height=100)

        default_prompt = "[Question]\\n{question}\\n\\në‹µë³€ì€ í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ì„¸ìš”."
        st.markdown("ê³µí†µ í”„ë¡¬í”„íŠ¸ (ë¯¸ìž…ë ¥ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©)")
        prompt_val = st.text_area(
            "í”„ë¡¬í”„íŠ¸",
            key="prompt_common",
            value=default_prompt,
            placeholder="ì˜ˆ: [Question]\\n{question}\\n\\në‹µë³€ì€ í•œêµ­ì–´ë¡œ ìž‘ì„±í•˜ì„¸ìš”.",
            height=120,
        )
        st.markdown("ì„ íƒì‚¬í•­: ì˜ˆì‹œ ë‹µë³€(ê¸°ì¤€)")
        st.text_area(
            "ì˜ˆì‹œ ë‹µë³€ (ì—†ìœ¼ë©´ ë¹„ì›Œë‘ì„¸ìš”)",
            key="prompt_eval_reference",
            placeholder="ì˜ˆì‹œ: ì£¼ 4ì¼ì œ ë„ìž… ì‹œ ìƒì‚°ì„±ì´ ìœ ì§€ë˜ê³  ì´ì§ë¥ ì´ ë‚®ì•„ì§„ ì‚¬ë¡€ë¥¼ ê·¼ê±°ë¡œ ì„¤ëª…...",
            height=120,
        )

        if st.button("í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹¤í–‰", disabled=not question_eval or not active_models):
            with st.spinner("í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹¤í–‰ ì¤‘..."):
                try:
                    active_labels = [MODEL_OPTIONS[k]["label"] for k in active_models if k in MODEL_OPTIONS]
                    prompt_payload = prompt_val.strip() or None
                    _send_prompt_eval(question_eval, eval_url, headers, prompt_payload, active_labels)
                except Exception as exc:  # pragma: no cover
                    st.error(f"ìš”ì²­ ì‹¤íŒ¨: {exc}")

    with tab_graph:
        st.header("ê·¸ëž˜í”„ ë³´ê¸° (Mermaid)")
        st.caption("LangGraph ì›Œí¬í”Œë¡œìš°ì™€ í”„ë¡¬í”„íŠ¸ í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ë‹¨ìˆœí™”í•´ í‘œì‹œí•©ë‹ˆë‹¤.")
        st.subheader("Chat Graph")
        chat_mermaid = """
        flowchart TD
          Q[User Question] --> INIT[init_question]
          INIT -->|fan-out| OAI[call_openai]
          INIT --> GEM[call_gemini]
          INIT --> ANT[call_anthropic]
          INIT --> PPLX[call_perplexity]
          INIT --> UPS[call_upstage]
          INIT --> MIS[call_mistral]
          INIT --> GRQ[call_groq]
          INIT --> COH[call_cohere]
          OAI --> END1[END]
          GEM --> END1
          ANT --> END1
          PPLX --> END1
          UPS --> END1
          MIS --> END1
          GRQ --> END1
          COH --> END1
        """
        st.markdown(f"```mermaid\n{chat_mermaid}\n```")

        st.subheader("Prompt Eval")
        eval_mermaid = """
        flowchart TD
          Q2[Question + Common Prompt] --> GEN[Generate: each active model]
          GEN --> OAI_E[Eval by OpenAI (latest)]
          GEN --> GEM_E[Eval by Gemini (latest)]
          GEN --> ANT_E[Eval by Anthropic (latest)]
          GEN --> UPS_E[Eval by Upstage (latest)]
          GEN --> PPLX_E[Eval by Perplexity (latest)]
          GEN --> MIS_E[Eval by Mistral (latest)]
          GEN --> GRQ_E[Eval by Groq (latest)]
          GEN --> COH_E[Eval by Cohere (latest)]
          OAI_E --> SUM[Summary (scores, rationales)]
          GEM_E --> SUM
          ANT_E --> SUM
          UPS_E --> SUM
          PPLX_E --> SUM
          MIS_E --> SUM
          GRQ_E --> SUM
          COH_E --> SUM
        """
        st.markdown(f"```mermaid\n{eval_mermaid}\n```")


if __name__ == "__main__":
    main()
